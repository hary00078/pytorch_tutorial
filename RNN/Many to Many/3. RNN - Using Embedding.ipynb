{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'purple'>\"Repeat is the best medicine for\"을 입력받으면 \"is the best medicine for memory\"를 출력하는 RNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 훈련 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Repeat', 'is', 'the', 'best', 'medicine', 'for', 'memory']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Repeat is the best medicine for memory\".split()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Repeat', 'the', 'for', 'memory', 'best', 'medicine', 'is']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(sentence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Repeat': 1, 'the': 2, 'for': 3, 'memory': 4, 'best': 5, 'medicine': 6, 'is': 7, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "word2index = {word : idx+1 for idx, word in enumerate(vocab)}\n",
    "word2index[\"<unk>\"] = 0\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Repeat', 2: 'the', 3: 'for', 4: 'memory', 5: 'best', 6: 'medicine', 7: 'is', 0: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "index2word = {idx : word for word, idx in word2index.items()}\n",
    "print(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 각 단어를 정수로 인코딩하는 동시에, 입력 데이터와 레이블 데이터를 만드는 함수\n",
    "def buil_data(sentence, word2index):\n",
    "    encoded = [word2index[word] for word in sentence]\n",
    "    input_seq, label_seq = encoded[:-1], encoded[1:]\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0)\n",
    "    label_seq = torch.LongTensor(label_seq).unsqueeze(0)\n",
    "    return input_seq, label_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 7, 2, 5, 6, 3]])\n",
      "tensor([[7, 2, 5, 6, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "X, Y = buil_data(sentence, word2index)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=input_size)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.embedding_layer(x)\n",
    "        output, hidden = self.rnn(output)\n",
    "        output = self.linear(output)\n",
    "        return output.view(-1, output.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2index) # <unk> 포함\n",
    "input_size = 5\n",
    "hidden_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(vocab_size, input_size, hidden_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "nb_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1510, -0.1057, -0.2058,  0.0395, -0.0247, -0.2041, -0.0230,  0.0372],\n",
      "        [ 0.0719,  0.2798, -0.0633,  0.4156,  0.3582,  0.1935,  0.1251,  0.2459],\n",
      "        [ 0.0893,  0.2253, -0.1257, -0.0484,  0.1712,  0.2547,  0.0961,  0.3699],\n",
      "        [-0.0636, -0.0543, -0.0725, -0.0672, -0.3806, -0.0295, -0.1496, -0.0218],\n",
      "        [-0.0685,  0.2305,  0.0437,  0.3823,  0.1842,  0.0773,  0.0769,  0.3030],\n",
      "        [ 0.1726, -0.0654, -0.2509, -0.0884,  0.2205, -0.0285,  0.0900,  0.0700]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "torch.Size([6, 8])\n"
     ]
    }
   ],
   "source": [
    "output = model(X)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 loss :\n",
      " tensor(0.0351, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 20 loss :\n",
      " tensor(0.0321, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 40 loss :\n",
      " tensor(0.0295, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 60 loss :\n",
      " tensor(0.0272, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 80 loss :\n",
      " tensor(0.0252, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 100 loss :\n",
      " tensor(0.0234, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 120 loss :\n",
      " tensor(0.0218, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 140 loss :\n",
      " tensor(0.0203, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 160 loss :\n",
      " tensor(0.0190, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 180 loss :\n",
      " tensor(0.0179, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n",
      "epoch : 200 loss :\n",
      " tensor(0.0168, grad_fn=<NllLossBackward>) result : tensor([7, 2, 5, 6, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs+1):\n",
    "    output = model(X)\n",
    "    loss = loss_function(output, Y.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        results = output.argmax(dim = 1)\n",
    "        print(\"epoch :\", epoch, \"loss :\\n\", loss, \"result :\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the best medicine for memory\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([index2word[idx] for idx in results.tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
